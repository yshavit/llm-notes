# Putting it all together

:::{warning} WIP
TODO

In addition to whatever's in the book, make sure I cover:

1. Residual connections / skip connections: The attention output gets added back to the input (x + Attention(x)). This is crucial for training deep networks.
2. Layer normalization: Usually applied before or after attention (or both). This is important for training stability.
:::
