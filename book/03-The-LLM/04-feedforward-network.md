# Feedforward network

:::{warning} WIP
TODO

Also include something like:

> :::{note}
> **Activation vs. Activation Function**
>
> Don't confuse these two related terms:
>
> - **Activation**: The numeric value at a neuron (e.g., 3.7 or 0 or -2.1)
> - **Activation function**: The mathematical function that computes that value (e.g., ReLU, sigmoid, tanh)
>
> For example, if a neuron receives input -1.5 and applies the ReLU function (which outputs max(0, x)), the resulting **activation** is 0.
>
> The term "activation" comes from biological neurons that either "fire" or don't, but in artificial neural networks, activations are continuous numeric values, not just on/off states.
> :::
:::
