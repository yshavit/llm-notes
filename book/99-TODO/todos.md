# Random TODOs

I should make sure I cover these before I call this project done!

- Layer normalization - Most transformer architectures include layer norm before/after attention, and this is crucial for training stability. Even a brief mention would be valuable.

- Residual connections - The skip connection around the attention block is fundamental to how deep transformers train. Without it, you can't effectively stack multiple layers.

- unified system for callouts: `{note}`, `{important}`, etc. These should be cohesive.
