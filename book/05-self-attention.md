# Self-attention

## What and _why_ is self-attention?

In [the previous section](./04-input-to-vectors), I described how to turn input text into a list of vectors. In the next section, we'll be feeding those vectors into a neural network. But first, in this section, we're going to use a process called {dfn}`self-attention` to determine how much each word in the input affects the other words in the input.

{drawio}`images/05/llm-flow-self-attention`

:::{warning} WIP
TODO
:::
